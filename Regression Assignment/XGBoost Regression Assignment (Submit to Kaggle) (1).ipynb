{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is inspired by: \n",
    "\n",
    "- https://www.kaggle.com/code/carlmcbrideellis/an-introduction-to-xgboost-regression\n",
    "- https://www.kaggle.com/code/dansbecker/xgboost/notebook\n",
    "\n",
    "In this assignment we will apply XGBoost Regression techniques to predict house prices, based on the famous Kaggle Dataset https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques\n",
    "\n",
    "Step 1 is to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "execution": {
     "iopub.execute_input": "2022-10-17T08:49:03.875064Z",
     "iopub.status.busy": "2022-10-17T08:49:03.874451Z",
     "iopub.status.idle": "2022-10-17T08:49:03.951957Z",
     "shell.execute_reply": "2022-10-17T08:49:03.950940Z",
     "shell.execute_reply.started": "2022-10-17T08:49:03.874981Z"
    }
   },
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "# load up the libraries\n",
    "#=========================================================================\n",
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "import xgboost as xgb\n",
    "\n",
    "#=========================================================================\n",
    "# read in the data\n",
    "#=========================================================================\n",
    "train_data = pd.read_csv(\"C:\\\\Users\\\\17789\\\\LHL\\\\work\\\\XGBOOST\\\\Regression Assignment\\\\train.csv\",index_col=0)\n",
    "test_data  = pd.read_csv(\"C:\\\\Users\\\\17789\\\\LHL\\\\work\\\\XGBOOST\\\\Regression Assignment\\\\test.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"width:60%;\">Feature selection</center>\n",
    "The purpose of feature selection, as the name suggests, is to only model the most pertinent and important features, thus reducing the computational overhead, and also to alleviate the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). The following are a number of notebooks covering techniques to achieve said goal, all of which use the House Prices data as an example:\n",
    "\n",
    "* [Feature selection using the Boruta-SHAP package](https://www.kaggle.com/carlmcbrideellis/feature-selection-using-the-boruta-shap-package)\n",
    "* [Recursive Feature Elimination (RFE) example](https://www.kaggle.com/carlmcbrideellis/recursive-feature-elimination-rfe-example)\n",
    "* [House Prices: Permutation Importance example](https://www.kaggle.com/carlmcbrideellis/house-prices-permutation-importance-example)\n",
    "* [Feature importance using the LASSO](https://www.kaggle.com/carlmcbrideellis/feature-importance-using-the-lasso)\n",
    "\n",
    "In this assignment, we shall use all of the numerical columns, and ignore the categorical features. To encode the categorical features one can use for example [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html). \n",
    "\n",
    "Our first task is to do Feature Exploration and Selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\17789\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSelectKBest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\17789\\LHL\\work\\XGBOOST\\Regression Assignment\\XGBoost Regression Assignment (Submit to Kaggle) (1).ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/17789/LHL/work/XGBOOST/Regression%20Assignment/XGBoost%20Regression%20Assignment%20%28Submit%20to%20Kaggle%29%20%281%29.ipynb#W3sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Feature Selection\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/17789/LHL/work/XGBOOST/Regression%20Assignment/XGBoost%20Regression%20Assignment%20%28Submit%20to%20Kaggle%29%20%281%29.ipynb#W3sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# SelectKBest to select the top features based on f_regression\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/17789/LHL/work/XGBOOST/Regression%20Assignment/XGBoost%20Regression%20Assignment%20%28Submit%20to%20Kaggle%29%20%281%29.ipynb#W3sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m selector \u001b[39m=\u001b[39m SelectKBest(score_func\u001b[39m=\u001b[39mf_regression, k\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/17789/LHL/work/XGBOOST/Regression%20Assignment/XGBoost%20Regression%20Assignment%20%28Submit%20to%20Kaggle%29%20%281%29.ipynb#W3sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m selector\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/17789/LHL/work/XGBOOST/Regression%20Assignment/XGBoost%20Regression%20Assignment%20%28Submit%20to%20Kaggle%29%20%281%29.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Get the p-values for the feature scores\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/17789/LHL/work/XGBOOST/Regression%20Assignment/XGBoost%20Regression%20Assignment%20%28Submit%20to%20Kaggle%29%20%281%29.ipynb#W3sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m p_values \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39mpvalues_\n",
      "File \u001b[1;32mc:\\Users\\17789\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:467\u001b[0m, in \u001b[0;36m_BaseFilter.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run score function on (X, y) and get the appropriate features.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \n\u001b[0;32m    451\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 467\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m    468\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m], multi_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    469\u001b[0m )\n\u001b[0;32m    471\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params(X, y)\n\u001b[0;32m    472\u001b[0m score_func_ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore_func(X, y)\n",
      "File \u001b[1;32mc:\\Users\\17789\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\17789\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\17789\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\17789\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nSelectKBest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Identify categorical columns (assuming they are of type 'object')\n",
    "categorical_cols = train_data.columns[train_data.dtypes == 'object']\n",
    "\n",
    "# Create the encoder.\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the categorical data\n",
    "encoder.fit(train_data[categorical_cols])\n",
    "\n",
    "# Transform and replace the categorical columns in the training and test data with their one-hot encoded equivalents\n",
    "train_encoded = encoder.transform(train_data[categorical_cols])\n",
    "test_encoded = encoder.transform(test_data[categorical_cols])\n",
    "\n",
    "# Convert the encoded features into a DataFrame with appropriate column names\n",
    "encoded_cols = list(encoder.get_feature_names_out(categorical_cols))\n",
    "train_encoded_df = pd.DataFrame(train_encoded, index=train_data.index, columns=encoded_cols)\n",
    "test_encoded_df = pd.DataFrame(test_encoded, index=test_data.index, columns=encoded_cols)\n",
    "\n",
    "# Drop the original categorical columns and concatenate the encoded features\n",
    "train_data = pd.concat([train_data.drop(categorical_cols, axis=1), train_encoded_df], axis=1)\n",
    "test_data = pd.concat([test_data.drop(categorical_cols, axis=1), test_encoded_df], axis=1)\n",
    "\n",
    "# # Feature Exploration\n",
    "# # Explore the first few rows of the data to understand the feature columns\n",
    "# print(train_data.head())\n",
    "\n",
    "# # Describe the statistical characteristics of the numerical features\n",
    "# print(train_data.describe())\n",
    "\n",
    "# # Visualization of feature distributions and relationships\n",
    "# # Histograms or boxplots for feature distributions\n",
    "# train_data.hist(bins=50, figsize=(20,15))\n",
    "# plt.show()\n",
    "\n",
    "# # Correlation matrix heatmap\n",
    "# corr_matrix = train_data.corr()\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
    "# plt.show()\n",
    "\n",
    "# # Pairwise relationships\n",
    "# sns.pairplot(train_data)\n",
    "# plt.show()\n",
    "\n",
    "# Assume that the last column of the training data is the target variable\n",
    "X_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "\n",
    "# Feature Selection\n",
    "# SelectKBest to select the top features based on f_regression\n",
    "selector = SelectKBest(score_func=f_regression, k='all')\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the p-values for the feature scores\n",
    "p_values = selector.pvalues_\n",
    "\n",
    "# Select features that have a significant p-value\n",
    "significant_features = X_train.columns[p_values < 0.05]\n",
    "\n",
    "print(\"Significant features based on p-values:\", significant_features)\n",
    "\n",
    "# Use the selected features for modeling\n",
    "X_train_selected = X_train[significant_features]\n",
    "X_test_selected = test_data[significant_features]\n",
    "\n",
    "# Output the selected features dataset head\n",
    "print(X_train_selected.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"width:60%;\">Feature engineering</center>\n",
    "As mentioned, one aspect of feature engineering is the creation of new features out of existing features. A simple example would be to create a new feature which is the sum of the number of bathrooms in the house:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-17T08:49:03.999750Z",
     "iopub.status.busy": "2022-10-17T08:49:03.999440Z",
     "iopub.status.idle": "2022-10-17T08:49:04.014461Z",
     "shell.execute_reply": "2022-10-17T08:49:04.013575Z",
     "shell.execute_reply.started": "2022-10-17T08:49:03.999715Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\17789\\LHL\\work\\XGBOOST\\Regression Assignment\\XGBoost Regression Assignment (Submit to Kaggle) (1).ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/17789/LHL/work/XGBOOST/Regression%20Assignment/XGBoost%20Regression%20Assignment%20%28Submit%20to%20Kaggle%29%20%281%29.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m (X_train, X_test):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/17789/LHL/work/XGBOOST/Regression%20Assignment/XGBoost%20Regression%20Assignment%20%28Submit%20to%20Kaggle%29%20%281%29.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mn_bathrooms\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mBsmtFullBath\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m (df[\u001b[39m\"\u001b[39m\u001b[39mBsmtHalfBath\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m0.5\u001b[39m) \u001b[39m+\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mFullBath\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m (df[\u001b[39m\"\u001b[39m\u001b[39mHalfBath\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/17789/LHL/work/XGBOOST/Regression%20Assignment/XGBoost%20Regression%20Assignment%20%28Submit%20to%20Kaggle%29%20%281%29.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39marea_with_basement\u001b[39m\u001b[39m\"\u001b[39m]  \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mGrLivArea\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mTotalBsmtSF\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "for df in (X_train, X_test):\n",
    "    df[\"n_bathrooms\"] = df[\"BsmtFullBath\"] + (df[\"BsmtHalfBath\"]*0.5) + df[\"FullBath\"] + (df[\"HalfBath\"]*0.5)\n",
    "    df[\"area_with_basement\"]  = df[\"GrLivArea\"] + df[\"TotalBsmtSF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to apply some feature engineering to prepare for using the XGBoost Estimator to predict house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on this fascinating aspect may I recommend the free on-line book [\"*Feature Engineering and Selection: A Practical Approach for Predictive Models*\"](http://www.feat.engineering/) by Max Kuhn and Kjell Johnson.\n",
    "### <center style=\"background-color:Gainsboro; width:60%;\">XGBoost estimator</center>\n",
    "Note that for this competition we use the RMSLE evaluation metric, rather than the default metric, which for regression is the RMSE. For more on the peculiarities of the RMSLE see the Appendix below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "execution": {
     "iopub.execute_input": "2022-10-17T08:49:04.016433Z",
     "iopub.status.busy": "2022-10-17T08:49:04.015704Z",
     "iopub.status.idle": "2022-10-17T08:50:04.782101Z",
     "shell.execute_reply": "2022-10-17T08:50:04.781125Z",
     "shell.execute_reply.started": "2022-10-17T08:49:04.016384Z"
    }
   },
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "# XGBoost regression: \n",
    "# Parameters: \n",
    "# n_estimators  \"Number of gradient boosted trees. Equivalent to number \n",
    "#                of boosting rounds.\"\n",
    "# learning_rate \"Boosting learning rate (also known as “eta”)\"\n",
    "# max_depth     \"Maximum depth of a tree. Increasing this value will make \n",
    "#                the model more complex and more likely to overfit.\" \n",
    "#=========================================================================\n",
    "regressor=xgb.XGBRegressor(eval_metric='rmsle')\n",
    "\n",
    "#=========================================================================\n",
    "# exhaustively search for the optimal hyperparameters\n",
    "#=========================================================================\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# set up our search grid\n",
    "param_grid = {\"max_depth\":    [4, 5],\n",
    "              \"n_estimators\": [500, 600, 700],\n",
    "              \"learning_rate\": [0.01, 0.015]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you use grid search to find the optimal hyper parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## put code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters are \",search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you setup a XGBoost Regressor object using your hyperparameters and fit it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "execution": {
     "iopub.execute_input": "2022-10-17T08:51:42.696134Z",
     "iopub.status.busy": "2022-10-17T08:51:42.695498Z",
     "iopub.status.idle": "2022-10-17T08:51:43.901785Z",
     "shell.execute_reply": "2022-10-17T08:51:43.900931Z",
     "shell.execute_reply.started": "2022-10-17T08:51:42.696091Z"
    }
   },
   "outputs": [],
   "source": [
    "# put code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, can you run it on your test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Can you score your solution offline and see how it does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-17T08:51:44.533388Z",
     "iopub.status.busy": "2022-10-17T08:51:44.533094Z",
     "iopub.status.idle": "2022-10-17T08:51:44.548515Z",
     "shell.execute_reply": "2022-10-17T08:51:44.547390Z",
     "shell.execute_reply.started": "2022-10-17T08:51:44.533360Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in the ground truth file\n",
    "solution   = pd.read_csv(<your solution file>)\n",
    "y_true     = solution[\"SalePrice\"]\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "RMSLE = np.sqrt( mean_squared_log_error(y_true, predictions) )\n",
    "print(\"The score is %.5f\" % RMSLE )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the below block to prepare your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "execution": {
     "iopub.execute_input": "2022-10-17T08:50:06.964744Z",
     "iopub.status.busy": "2022-10-17T08:50:06.964340Z",
     "iopub.status.idle": "2022-10-17T08:50:06.977852Z",
     "shell.execute_reply": "2022-10-17T08:50:06.976712Z",
     "shell.execute_reply.started": "2022-10-17T08:50:06.964712Z"
    }
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\"Id\":test_data.index, \"SalePrice\":predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Feature importance</center>\n",
    "Let us also take a very quick look at the feature importance too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-17T08:50:06.980369Z",
     "iopub.status.busy": "2022-10-17T08:50:06.979530Z",
     "iopub.status.idle": "2022-10-17T08:50:07.282871Z",
     "shell.execute_reply": "2022-10-17T08:50:07.281929Z",
     "shell.execute_reply.started": "2022-10-17T08:50:06.980309Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "plot_importance(regressor, max_num_features=8, ax=ax)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where here the `F score` is a measure \"*...based on the number of times a variable is selected for splitting, weighted by the squared improvement to the model as a result of each split, and averaged over all trees*.\" [1] \n",
    "\n",
    "Note that these importances are susceptible to small changes in the training data, and it is much better to make use of [\"GPU accelerated SHAP values\"](https://www.kaggle.com/carlmcbrideellis/gpu-accelerated-shap-values-jane-street-example), incorporated with version 1.3 of XGBoost.\n",
    "\n",
    "Can you follow the above guide use SHAP values instead of F Score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Appendix: The RMSLE evaluation metric</center>\n",
    "From the competition [evaluation page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) we see that the metric we are using is the root mean squared logarithmic error (RMSLE), which is given by\n",
    "\n",
    "$$ {\\mathrm {RMSLE}}\\,(y, \\hat y) = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2} $$\n",
    "\n",
    "where $\\hat{y}_i$ is the predicted value of the target for instance $i$, and $y_i$\n",
    "is the actual value of the target for instance $i$.\n",
    "\n",
    "It is important to note that, unlike the RMSE, the RMSLE is asymmetric; penalizing much more the underestimated predictions than the overestimated predictions. For example, say the correct value is $y_i = 1000$, then underestimating by 600 is almost twice as bad as overestimating by 600:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-17T08:50:07.284616Z",
     "iopub.status.busy": "2022-10-17T08:50:07.284340Z",
     "iopub.status.idle": "2022-10-17T08:50:07.290854Z",
     "shell.execute_reply": "2022-10-17T08:50:07.289987Z",
     "shell.execute_reply.started": "2022-10-17T08:50:07.284584Z"
    }
   },
   "outputs": [],
   "source": [
    "def RSLE(y_hat,y):\n",
    "    return np.sqrt((np.log1p(y_hat) - np.log1p(y))**2)\n",
    "\n",
    "print(\"The RMSLE score is %.3f\" % RSLE( 400,1000) )\n",
    "print(\"The RMSLE score is %.3f\" % RSLE(1600,1000) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asymmetry arises because \n",
    "\n",
    "$$ \\log (1 + \\hat{y}_i) - \\log (1 + y_i) =  \\log \\left( \\frac{1 + \\hat{y}_i}{1 + y_i} \\right) $$\n",
    "\n",
    "so we are essentially looking at ratios, rather than differences such as is the case of the RMSE. We can see the form that this asymmetry takes in the following plot, again using 1000 as our ground truth value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-17T08:50:07.292446Z",
     "iopub.status.busy": "2022-10-17T08:50:07.292191Z",
     "iopub.status.idle": "2022-10-17T08:50:07.467273Z",
     "shell.execute_reply": "2022-10-17T08:50:07.466163Z",
     "shell.execute_reply.started": "2022-10-17T08:50:07.292407Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
    "x = np.linspace(5,4000,100)\n",
    "plt.plot(x, RSLE(x,1000))\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('RMSLE')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
